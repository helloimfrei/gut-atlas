{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4789b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "import re\n",
    "from gutatlas.data import filter_by_tag\n",
    "from gutatlas.utils.constants import MENTAL_HEALTH_TAGS, GI_TAGS\n",
    "from gutatlas.data import map_gi_status_binary, normalize_multilabel_gi_tags\n",
    "from gutatlas.features import clean_feature_names\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577833d",
   "metadata": {},
   "source": [
    "## Build Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_path = \"../data/raw/taxonomic_table.csv\"\n",
    "metadata_path = \"../data/raw/sample_metadata.tsv\"\n",
    "tags_path = \"../data/raw/tags.tsv\"\n",
    "\n",
    "sample_metadata = pl.read_csv(metadata_path, separator=\"\\t\").with_columns(\n",
    "    (pl.col(\"project\") + \"_\" + pl.col(\"srr\")).alias(\"sample\")\n",
    ")\n",
    "sample_tags = pl.read_csv(tags_path, separator=\"\\t\").with_columns(\n",
    "    (pl.col(\"project\") + \"_\" + pl.col(\"srr\")).alias(\"sample\")\n",
    ")\n",
    "\n",
    "#taxon table is huge. open batches of 1000, normalize the abundances of each taxon on a per-sample basis\n",
    "#then save each batch for easier processing going forward\n",
    "reader = pl.read_csv_batched(taxon_path, batch_size=1000)\n",
    "first_batch = True\n",
    "i = 0\n",
    "while True:\n",
    "    batches = reader.next_batches(1)\n",
    "    if not batches:\n",
    "        break\n",
    "    batch = batches[0]\n",
    "    taxon_cols = batch.columns[2:]\n",
    "\n",
    "    batch = batch.with_columns(\n",
    "        pl.sum_horizontal([pl.col(col) for col in taxon_cols]).alias(\"total_reads\")\n",
    "    ).with_columns([\n",
    "        (pl.col(col) / pl.col(\"total_reads\")).alias(col) for col in taxon_cols\n",
    "    ]).select([\"sample\"] + taxon_cols)\n",
    "\n",
    "    merged = (\n",
    "        batch.join(sample_metadata, on=\"sample\", how=\"inner\")\n",
    "             .join(sample_tags, on=\"sample\", how=\"left\")\n",
    "             .drop([\"project_right\", \"srr_right\", \"srs_right\", \"total_bases\", \n",
    "                    \"instrument\",'srs', 'project', 'srr', 'library_strategy', 'library_source'])\n",
    "    )\n",
    "\n",
    "    merged.write_parquet(f\"../data/interim/batches/taxa_merged_batch_{i}.parquet\")\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#lazily read all batches, split by region, and save each region separately\n",
    "batches = pl.scan_parquet('../data/interim/batches/taxa_merged_batch_*.parquet')\n",
    "\n",
    "unique_regions = (\n",
    "    batches.select(\"iso\")\n",
    "           .unique()\n",
    "           .collect()\n",
    ")\n",
    "\n",
    "for region in unique_regions['iso']:\n",
    "    split = batches.filter(pl.col('iso') == region).collect()\n",
    "\n",
    "    split.write_parquet(f'../data/interim/regional_data/{region}_microbiome.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab31f90",
   "metadata": {},
   "source": [
    "### GI tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06607388",
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_merged = filter_by_tag('../data/interim/regional_data/',GI_TAGS)\n",
    "gi_merged.write_parquet('../data/interim/filtered_and_merged/gi_microbiomes_merged.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9fd7b3",
   "metadata": {},
   "source": [
    "#### binary classification set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c64c1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_cols = pd.read_csv('../data/processed/inactive_columns.csv')\n",
    "#remove duplicate rows for each sample. only need to know if any disease is present or not \n",
    "merged_gi = pd.read_parquet('../data/interim/filtered_and_merged/gi_microbiomes_merged.parquet')\n",
    "merged_gi['disease_present'] = merged_gi.value.apply(map_gi_status_binary)\n",
    "\n",
    "gi_training = (merged_gi\n",
    "                         .sort_values(by = ['disease_present','sample'],ascending=False)\n",
    "                         .drop_duplicates(subset = 'sample',keep='first')\n",
    "                         .reset_index(drop=True)\n",
    "                         .drop(columns = ['pubdate','geo_loc_name','iso','region','tag','value'])\n",
    "                         .rename(columns = {col:clean_feature_names(col) for col in merged_gi.columns})\n",
    "                         .clean_names()\n",
    "                         )\n",
    "\n",
    "\n",
    "gi_training.to_parquet('../data/processed/gi_binary_training.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921ebd9",
   "metadata": {},
   "source": [
    "#### multilabel classification set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecb6c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gi = pd.read_parquet('../data/interim/filtered_and_merged/gi_microbiomes_merged.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6fa6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_gi_tags = normalize_multilabel_gi_tags(merged_gi)\n",
    "merged_gi_normalized = pd.merge(left = merged_gi.drop(columns=['tag','value']), right= normalized_gi_tags, on = 'sample', how = 'left')\n",
    "#keep disease classes specific\n",
    "merged_gi_normalized = merged_gi_normalized[merged_gi_normalized.tag != 'GI_other'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94f5840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabels = merged_gi_normalized.pivot_table(index = 'sample',columns='tag',values='value', fill_value=0).reset_index()\n",
    "\n",
    "gi_multilabel_training = (pd.merge(multilabels,merged_gi_normalized.drop_duplicates('sample'),'left','sample')\n",
    "                          .drop(columns = ['pubdate','geo_loc_name','iso','region','tag','value'])\n",
    "                          .clean_names()\n",
    ")\n",
    "gi_multilabel_training.to_parquet('../data/processed/gi_multilabel_training.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a338866",
   "metadata": {},
   "source": [
    "### Mental health tags. ended up being too few positives for use in modelling unfortunately :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb45ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_merged = filter_by_tag('../data/interim/regional_data/',MENTAL_HEALTH_TAGS)\n",
    "mental_health_merged.write_parquet('../data/interim/filtered_and_merged/mental_health_microbiomes_merged.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1810ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i'm only interested in specific mental illnesses, not general/mixed values. I'm dropping anything vague/general thats mapped to -1\n",
    "#i want to use this for multilabel classification\n",
    "\n",
    "tag_mapping = {\n",
    "    # Core combined\n",
    "    \"depression_bipolar_schizophrenia\": -1,\n",
    "\n",
    "    # General\n",
    "    \"mental_illness\": -1,\n",
    "    \"mental_illness_type\": -1,\n",
    "\n",
    "    # Specific disorders\n",
    "    \"mental_illness_type_anorexia_nervosa\": \"anorexia\",\n",
    "    \"mental_illness_type_bipolar_disorder\": \"bipolar_disorder\",\n",
    "    \"mental_illness_type_bulimia_nervosa\": \"bulimia\",\n",
    "    \"mental_illness_type_depression\": \"depression\",\n",
    "    \"mental_illness_type_schizophrenia\": \"schizophrenia\",\n",
    "    \"mental_illness_type_substance_abuse\": \"substance_abuse\",\n",
    "    \"mental_illness_type_unspecified\": -1,\n",
    "\n",
    "    # PTSD variants (normalize both spellings)\n",
    "    \"mental_illness_type_ptsd_posttraumatic_stress_disorder\": \"ptsd\",\n",
    "    \"mental_illness_type_ptsd_post_traumatic_stress_disorder\": \"ptsd\",\n",
    "\n",
    "    # Keep both depression-related signals\n",
    "    \"has_depression1\": \"depression\",\n",
    "    \"has_depression2\": \"depression\",\n",
    "\n",
    "    #i wasn't able to locate what scale was being used for these numerical tags, so i'm dropping them\n",
    "    \"depression_index1\": -1,\n",
    "    \"depression_index2\": -1,\n",
    "\n",
    "    \"anxiety_index1\": -1,\n",
    "    \"anxiety_index2\": -1,\n",
    "\n",
    "    \"depression_level\": -1,\n",
    "    \"depression_status\": -1,\n",
    "    \"stress_level\": -1,\n",
    "    \"stress_status\": -1,\n",
    "}\n",
    "\n",
    "value_mapping = {\n",
    "    'false':0,\n",
    "    'no':0,\n",
    "    'not provided':-1,\n",
    "    'i do not have this condition':0,\n",
    "    'self-diagnosed':1,\n",
    "    'not collected':-1,\n",
    "    'diagnosed by a medical professional (doctor, physician assistant)':1,\n",
    "    'labcontrol test':1,\n",
    "    'unspecified':-1\n",
    "}\n",
    "\n",
    "def bin_promis_scale(score):\n",
    "    # https://www.sciencedirect.com/science/article/abs/pii/S0889159119315314?via%3Dihub\n",
    "    #based on the above paper (where the promis score tag/values originated), under 21 and below is a good threshold for binary binning\n",
    "    try:\n",
    "        score =  int(score)\n",
    "        return 0 if score < 22 else 1\n",
    "    except (ValueError, TypeError):\n",
    "        return score\n",
    "    \n",
    "mental_health_merged = pd.read_parquet('../data/interim/filtered_and_merged/mental_health_microbiomes_merged.parquet')\n",
    "mental_health_merged.value = mental_health_merged.value.apply(bin_promis_scale)\n",
    "mental_health_merged.tag = mental_health_merged.tag.map(tag_mapping)\n",
    "mental_health_merged.value = mental_health_merged.value.map(value_mapping)\n",
    "mental_health_merged = (mental_health_merged[~((mental_health_merged.tag == -1) | (mental_health_merged.value == -1))]\n",
    "                        .dropna(how='any')\n",
    "                        .reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b93be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 1 of 69\n",
      "finished 0\n",
      "starting 2 of 69\n",
      "finished 1\n",
      "starting 3 of 69\n",
      "finished 2\n",
      "starting 4 of 69\n",
      "finished 3\n",
      "starting 5 of 69\n",
      "finished 4\n",
      "starting 6 of 69\n",
      "finished 5\n",
      "starting 7 of 69\n",
      "finished 6\n",
      "starting 8 of 69\n",
      "finished 7\n",
      "starting 9 of 69\n",
      "finished 8\n",
      "starting 10 of 69\n",
      "finished 9\n",
      "starting 11 of 69\n",
      "finished 10\n",
      "starting 12 of 69\n",
      "finished 11\n",
      "starting 13 of 69\n",
      "finished 12\n",
      "starting 14 of 69\n",
      "finished 13\n",
      "starting 15 of 69\n",
      "finished 14\n",
      "starting 16 of 69\n",
      "finished 15\n",
      "starting 17 of 69\n",
      "finished 16\n",
      "starting 18 of 69\n",
      "finished 17\n",
      "starting 19 of 69\n",
      "finished 18\n",
      "starting 20 of 69\n",
      "finished 19\n",
      "starting 21 of 69\n",
      "finished 20\n",
      "starting 22 of 69\n",
      "finished 21\n",
      "starting 23 of 69\n",
      "finished 22\n",
      "starting 24 of 69\n",
      "finished 23\n",
      "starting 25 of 69\n",
      "finished 24\n",
      "starting 26 of 69\n",
      "finished 25\n",
      "starting 27 of 69\n",
      "finished 26\n",
      "starting 28 of 69\n",
      "finished 27\n",
      "starting 29 of 69\n",
      "finished 28\n",
      "starting 30 of 69\n",
      "finished 29\n",
      "starting 31 of 69\n",
      "finished 30\n",
      "starting 32 of 69\n",
      "finished 31\n",
      "starting 33 of 69\n",
      "finished 32\n",
      "starting 34 of 69\n",
      "finished 33\n",
      "starting 35 of 69\n",
      "finished 34\n",
      "starting 36 of 69\n",
      "finished 35\n",
      "starting 37 of 69\n",
      "finished 36\n",
      "starting 38 of 69\n",
      "finished 37\n",
      "starting 39 of 69\n",
      "finished 38\n",
      "starting 40 of 69\n",
      "finished 39\n",
      "starting 41 of 69\n",
      "finished 40\n",
      "starting 42 of 69\n",
      "finished 41\n",
      "starting 43 of 69\n",
      "finished 42\n",
      "starting 44 of 69\n",
      "finished 43\n",
      "starting 45 of 69\n",
      "finished 44\n",
      "starting 46 of 69\n",
      "finished 45\n",
      "starting 47 of 69\n",
      "finished 46\n",
      "starting 48 of 69\n",
      "finished 47\n",
      "starting 49 of 69\n",
      "finished 48\n",
      "starting 50 of 69\n",
      "finished 49\n",
      "starting 51 of 69\n",
      "finished 50\n",
      "starting 52 of 69\n",
      "finished 51\n",
      "starting 53 of 69\n",
      "finished 52\n",
      "starting 54 of 69\n",
      "finished 53\n",
      "starting 55 of 69\n",
      "finished 54\n",
      "starting 56 of 69\n",
      "finished 55\n",
      "starting 57 of 69\n",
      "finished 56\n",
      "starting 58 of 69\n",
      "finished 57\n",
      "starting 59 of 69\n",
      "finished 58\n",
      "starting 60 of 69\n",
      "finished 59\n",
      "starting 61 of 69\n",
      "finished 60\n",
      "starting 62 of 69\n",
      "finished 61\n",
      "starting 63 of 69\n",
      "finished 62\n",
      "starting 64 of 69\n",
      "finished 63\n",
      "starting 65 of 69\n",
      "finished 64\n",
      "starting 66 of 69\n",
      "finished 65\n",
      "starting 67 of 69\n",
      "finished 66\n",
      "starting 68 of 69\n",
      "finished 67\n",
      "starting 69 of 69\n",
      "finished 68\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "merged = pd.DataFrame()\n",
    "batch_dir = os.listdir('../data/interim/regional_data/')\n",
    "for idx,batch in enumerate(batch_dir):\n",
    "    print('starting',idx+1,'of',len(batch_dir))\n",
    "    df = pd.read_parquet(f'../data/interim/regional_data/{batch}')\n",
    "    merged = pd.concat([merged,df[['tag','value']]],ignore_index=True)\n",
    "    del df\n",
    "    print('finished',idx+1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
